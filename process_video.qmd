---
title: "研究室用：動画エントロピーの解析"
subtitle: "エントロピーは計算済み"
date: "2022-09-08"
author: "Greg Nishihara"
format: 
  html:
    self-contained: true
    output-dir: docs
---

> エントロピーは計算済みです。
> データファイルはそれぞれの動画フォルダのサブフォルダに入っている。

## 必要なパッケージ

```{r}
library(tidyverse)
library(magick)
library(ggvegan)
library(ggpubr)
library(patchwork)
library(showtext)
library(lubridate)
library(vegan)
library(gnnlab)

# Python の numpy モジュールを読み込む
library(reticulate)
```

Python のモジュールはここで準備する。

```{r}
np = import("numpy")
```

## エントロピーとは

エントロピー  $(S)$ は **乱雑さ** や **不規則さ** を表現する情報の尺度です。
熱力学、生態学、情報理論に応用されています。

生態学のシャノン指数 $(H')$ はエントロピーと同じ用に求めます。

$$
S = H' = -\sum_{i=1}^{R} p_i\,ln( p_i)
$$

$p_i$ は $R$ （種数） 種の集まりにおける $i$種 の割合です。
このとき、`ln()` は底は `e` ですが、動画の解析には $ln() = log_2()$ としました。
種の割合がすべての同じの場合 $(p_i = 1/R)$、$S = ln(R)$ です。

**例 1**

```{r}
n = c(2, 2, 2, 2, 2, 2) # ６種、各種２個体
pi = n / sum(n)
-sum(pi * log(pi)) # シャノン指数の計算
log(length(n)) # 種数R の ln()
```

各種は均一に存在するので、種数のログ変換とシャノン指数の値は等しいです。
つまり、各種は同じ程度存在していることは、エントロピーが最大になることです。

**例 2**

```{r}
set.seed(2022)
n = sample(1:6, size = 6, replace = TRUE) # ６種、各種の個数は無作為で決める
pi = n / sum(n)
-sum(pi * log(pi)) # シャノン指数の計算
log(length(n)) # R
```

各種の個体数は不均一なので、シャノン指数は種数のログより低いです。

**例 3**

```{r}
set.seed(2022)
n = c(200,2,2,2,2,2) # ６種、種1 は他の種の100倍存在する
pi = n / sum(n)
-sum(pi * log(pi)) # シャノン指数の計算
log(length(n)) # R
```

一種が優先する場合、シャノン指数は $ln(R)$ よりとても低いです。
つまり、乱雑さや不規則さが低いです。

動画解析の場合、グレーの値が **種** です。グレーの値の数が **個体数** です。
グレーは 0 から 254 の値をとります。

**例 4** 

```{r}
g = rep(1, 255) # 0 から 254 の値の出現数は 1 
pi = g / sum(g)
-sum(pi * log(pi)) # シャノン指数の計算
log(length(g)) # R
```

グレーの値は均一の出現するので、シャノン指数と $ln(R)$ は等しい。

**例 5** 

```{r}
width = 1920
height = 1080
g = sample(0:254, size = width * height, replace = TRUE)
g = table(g) # グレー値ごとの数
pi = g / sum(g)
-sum(pi * log(pi)) # シャノン指数の計算
log(length(g)) # R
```

`width` と `height` は画像の縦横のピクセル数です。
グレーの値はほぼ均一に出現したので、シャノン指数と $ln(R)$ は等しい。

**例 6** 

画像を読み込み、グレースケールに変換する。
```{r}
# 画像の解像度は 4000x3000 です。
i1 = image_read("~/Lab_Data/Photos/Pew/b3160069.jpg") # 岩の写真
i2 = image_read("~/Lab_Data/Photos/Pew/b7200033.jpg") # ノコギリモクの写真

i1 = i1 |> image_quantize(colorspace = "gray")
i2 = i2 |> image_quantize(colorspace = "gray")
```

```{r}
image_append(c(i1, i2), stack = TRUE) |> 
  image_resize("x500")
```

シャノン指数（エントロピー）はここで求める。

```{r}
mat1 = as.integer(i1[[1]]) |> as.numeric()
mat2 = as.integer(i2[[1]]) |> as.numeric()
g1 = table(mat1) # グレー値ごとの数
g2 = table(mat2) # グレー値ごとの数

pi1 = g1 / sum(g1)
pi2 = g2 / sum(g2)

s1 = -sum(pi1 * log(pi1)) # シャノン指数の計算
s2 = -sum(pi2 * log(pi2)) # シャノン指数の計算

r1 = log(length(g1)) # R
r2 = log(length(g2)) # R

c(s1,s2)
c(r1,r2)
```

$ln(R)$ の値は、解像度とグレー値の種類だけに依存するのです。
ノコギリモクの写真の方がグレー値の種類が多いので、`r2` は `r1` とより高いです。

`s1` と `s2` は異なりました。値が低いほどシャノン指数（エントロピー）が低いです。
岩の写真のほうが低いですね。これは、グレー値の数（種類）が偏っているからです。
ノコギリモクの写真の場合、様々なグレー値が存在しています。


```{r}
tibble(grp = c("岩", "藻"),
       data = list(as_tibble(g1) |> rename(grey = mat1), 
                   as_tibble(g2) |> rename(grey = mat2)) ) |> 
  unnest(data) |> 
  mutate(grey = as.numeric(grey)) |> 
  ggplot() + 
  geom_col(aes(x = grey, y = n, fill = grp),
           position = position_dodge2(),
          width = 1) +
  scale_x_continuous(breaks = seq(0, 254, by = 32),
                     limits = c(0, 254)) +
  scale_fill_viridis_d("画像", end = 0.8) +
  guides(fill = "none") +
  facet_wrap(vars(grp), ncol = 1)
```


下記の解析には、**デルタエントロピー** を計算して実施した。
**デルタエントロピー** は動画のフレームとフレームの違いを求めて計算した。
今回は、2 フレーム開けて計算した。

手順は次の通りです。

1. フレームをグレースケールに変換する。
1. $t$ と $t+3$ のフレームの差分をもとめる。
1. 差分の結果の絶対値をもとめる。
1. 差分の 255 を足して、値が 0 から 254 になるようにする。
1. 調整済み差分のエントロピーを求め、デルタエントロピーとする。

## 作図の設定

```{r}
font_add(family = "notosans", regular = "NotoSansCJKjp-Regular.otf")
# theme_gray(base_size = 10, base_family = "notosans") |> theme_set()
theme_pubr(base_size = 10, base_family = "notosans") |> theme_set()
showtext_auto()
```


## 関数の定義

Python のデータファイルはこの関数で読み込みます。
Python のデータファイルの拡張子は `npy` です。
```{r}
read_npy = function(fnames) {
  dset = np$load(fnames) # numpy の load() を使って、 npy ファイルを読み込む
  calculate_mean(dset) |> as_tibble()
}
```

エントロピーデータは配列の状態で処理した方が早いです。
tibble化したデータの処理時間は数十時間かかることもあるからです。
`apply()` の場合は、5秒から10秒で処理されます。

```{r}
calculate_mean = function(X) {
  submatrix_mean = function(x, zmat, ymat, xmat) {
    mean(zmat[x[1], ymat[x[2], ], xmat[x[3], ]])
  }
  tau = seq(1, dim(X)[1]) # コマ軸 (時間)
  y   = seq(1, dim(X)[2]) # y 軸
  x   = seq(1, dim(X)[3]) # x 軸
  # 画像は 6 x 6 に区分する
  xmat = matrix(x, nrow = 6, byrow = T)
  ymat = matrix(y, nrow = 6, byrow = T)
  egrid = expand.grid(tau, 1:nrow(xmat), 1:nrow(ymat))
  cbind(egrid, 
        value = apply(X = egrid, 
                      MARGIN = 1, 
                      FUN = submatrix_mean, 
                      zmat = X, 
                      ymat = ymat, 
                      xmat = xmat))
}
```

## データ読み込みの紹介

フォルダとサブフォルダを取得する。

```{r}
folder = "~/Lab_Data/sudar/movie/"
subfolders = dir(folder, full = TRUE, pattern = "arikawa_[0-9]{6}")
```

時刻データを読み込む。
重複してる場合もあるので、`distinct()` に渡す。
```{r}
YMD = "220728"
dtdata = str_subset(subfolders, YMD) |> 
  dir("datetimes.csv", full = TRUE) |> 
  read_csv() |> 
  distinct()
```


`npy` データを読み込む。

```{r}
fnames = str_subset(subfolders, YMD) |> 
  dir("delta_entropy_csvdata", full = TRUE) |> 
  dir("npy", full = TRUE)

dset = tibble(fnames) |> 
  mutate(data = map(fnames, read_npy))
```

`dtdata` と `dset` を結合する。

```{r}
dall = inner_join(
    mutate(dtdata, axis = str_extract(filename, "suda_*.*_[0-9]{6}")),
    mutate(dset, axis = str_extract(basename(fnames), "suda_*.*_[0-9]{6}")),
    by = "axis"
) |> select(-c(fnames, filename))

dall = dall |> mutate(tau = floor_date(starttime, "minutes"))
```

## データの作図

* `Var1` は時間軸（フレーム）
* `Var2` は y 軸
* `Var3` は x 軸
* `value` はデルタエントロピー


**デルタエントロピーの時系列図：総平均**

```{r}
tmp = dall |> 
  unnest(data) |> 
  group_by(tau) |>
  summarise(z = mean(value),
            s = sd(value))
ggplot(tmp) + 
  geom_point(aes(x = tau, y = z)) +
  geom_errorbar(aes(x = tau, ymin = z - s, ymax = z + s),
                width = 0)
```

**デルタエントロピーの時系列図：コマごとの平均**

```{r}
tmp = dall |> 
  unnest(data) |> 
  group_by(tau, Var2, Var3) |>
  summarise(z = mean(value),
            s = sd(value))
ggplot(tmp) + 
  geom_point(aes(x = tau, y = z)) +
  geom_errorbar(aes(x = tau, ymin = z - s, ymax = z + s),
                width = 0) +
  facet_grid(rows = vars(Var3),
             cols = vars(Var2))
```

**デルタエントロピーの時系列図**

```{r}
tmp = dall |> 
  unnest(data) |> 
  group_by(Var2, Var3) |>
  summarise(z = mean(value),
            s = sd(value))

T1 = "平均値"
T2 = "標準偏差"
p1 = ggplot(tmp) + geom_tile(aes(x = Var3, y = Var2, fill = z)) + scale_fill_viridis_c(end = 0.9) + labs(title = T1)
p2 = ggplot(tmp) + geom_tile(aes(x = Var3, y = Var2, fill = s)) + scale_fill_viridis_c(end = 0.9) + labs(title = T2)
p1 + p2 + plot_layout(ncol = 1)
```



